# Explicabilidade de Modelos Preditivos de CrÃ©dito com LIME

Modelos de machine learning sÃ£o amplamente utilizados em sistemas de anÃ¡lise de crÃ©dito bancÃ¡rio, uma vez que conseguem prever com alta precisÃ£o se um cliente Ã© um bom ou mau pagador. No entanto, esses modelos sÃ£o frequentemente criticados por atuarem como "caixas-pretas".

Em setores altamente regulados como o financeiro, Ã© essencial nÃ£o apenas decidir corretamente, mas tambÃ©m ser capaz de justificar cada decisÃ£o. Assim, o objetivo deste projeto Ã© aplicar tÃ©cnicas de Explainable AI (XAI), utilizando a biblioteca LIME, para tornar as previsÃµes de um modelo preditivo mais interpretÃ¡veis e transparentes.

# Modelo Preditivo Escolhido

Foi utilizado o modelo Random Forest, uma tÃ©cnica de ensemble que combina vÃ¡rias Ã¡rvores de decisÃ£o para obter resultados mais robustos e menos propensos a overfitting. A escolha do Random Forest se deu por sua capacidade de lidar com variÃ¡veis categÃ³ricas e numÃ©ricas, bem como por sua boa performance em tarefas de classificaÃ§Ã£o binÃ¡ria.

# Etapas:

Carregamento e nomeaÃ§Ã£o do dataset "Statlog (German Credit Data)" da UCI.

CodificaÃ§Ã£o de variÃ¡veis categÃ³ricas.

DivisÃ£o entre treino (80%) e teste (20%).

Treinamento do modelo RandomForestClassifier.

AvaliaÃ§Ã£o com classification_report (acurÃ¡cia ~81%).

# Explicabilidade com LIME

A biblioteca LIME (Local Interpretable Model-agnostic Explanations) foi utilizada para gerar explicaÃ§Ãµes locais sobre as previsÃµes do modelo. Com LIME, Ã© possÃ­vel identificar quais variÃ¡veis mais influenciaram a decisÃ£o de classificar um cliente como "bom pagador" ou "mau pagador".

Exemplos de saÃ­da:

GrÃ¡ficos com as features mais relevantes para cada previsÃ£o.

AnÃ¡lise textual com impacto positivo ou negativo de cada variÃ¡vel.

Essas explicaÃ§Ãµes permitem que analistas de crÃ©dito, clientes e Ã³rgÃ£os reguladores compreendam a decisÃ£o do modelo, aumentando a confianÃ§a no sistema automatizado.

# ReflexÃµes e LimitaÃ§Ãµes

Apesar das vantagens, a interpretabilidade local fornecida pelo LIME pode nÃ£o refletir toda a complexidade do modelo global. AlÃ©m disso, escolhas como o nÃºmero de vizinhos ou a granularidade da discretizaÃ§Ã£o influenciam diretamente a explicaÃ§Ã£o.

Ainda assim, o uso do LIME representa um avanÃ§o significativo rumo a modelos mais transparentes e auditÃ¡veis. Ao observar os fatores que pesaram contra ou a favor de um cliente, Ã© possÃ­vel tambÃ©m orientar melhorias nos dados ou no processo de decisÃ£o humana.

Com o avanÃ§o da InteligÃªncia Artificial, modelos preditivos tÃªm sido amplamente utilizados em decisÃµes sensÃ­veis, como a concessÃ£o de crÃ©dito. Embora precisos, esses modelos frequentemente funcionam como uma â€œcaixa-pretaâ€, dificultando a compreensÃ£o de suas decisÃµes por parte de clientes, gerentes e Ã³rgÃ£os regulatÃ³rios.

Este projeto utiliza a tÃ©cnica de **XAI (Explainable Artificial Intelligence)** com a biblioteca **LIME**, para tornar transparentes as decisÃµes de um modelo que classifica clientes como â€œbomâ€ ou â€œmauâ€ pagador.

---

# Objetivos

- Treinar um modelo de classificaÃ§Ã£o para risco de crÃ©dito.
- Aplicar o LIME para explicar as decisÃµes do modelo de forma individualizada.
- Promover a transparÃªncia do modelo preditivo e facilitar sua interpretaÃ§Ã£o por stakeholders.

---

#  Modelo Preditivo

Utilizamos o modelo **Random Forest Classifier**, uma tÃ©cnica de aprendizado de mÃ¡quina baseada em mÃºltiplas Ã¡rvores de decisÃ£o. Ele foi escolhido por oferecer bom desempenho e estabilidade em classificaÃ§Ãµes binÃ¡rias como essa.

- **Dados:** Statlog (German Credit Data) â€” UCI Repository
- **VariÃ¡veis:** 20 atributos como idade, conta bancÃ¡ria, histÃ³rico de crÃ©dito, renda, etc.
- **Classes:** 1 (bom pagador), 0 (mau pagador)
- **AcurÃ¡cia obtida:** ~81%

---

# TÃ©cnicas de Explicabilidade

O **LIME (Local Interpretable Model-agnostic Explanations)** foi usado para gerar explicaÃ§Ãµes locais â€” ou seja, explicar cada decisÃ£o do modelo individualmente. Isso Ã© feito analisando como pequenas mudanÃ§as nas variÃ¡veis influenciam a saÃ­da do modelo.

Exemplo de explicaÃ§Ã£o para um cliente:

- `CreditHistory > 2` âœ Aumentou chance de aprovaÃ§Ã£o
- `Status <= 3` âœ Diminuiu chance de aprovaÃ§Ã£o

GrÃ¡ficos foram gerados para mostrar visualmente as variÃ¡veis mais relevantes para cada cliente analisado.

---

# Outputs

Os principais outputs do projeto sÃ£o:

- ğŸ“Š **GrÃ¡fico com explicaÃ§Ãµes geradas pelo LIME**  
- ğŸ“‹ **Tabela textual com os impactos das variÃ¡veis**
- ğŸ§¾ **RelatÃ³rio de classificaÃ§Ã£o** com mÃ©tricas (precision, recall, f1-score)

 Veja a pasta `/imagens` com alguns exemplos.

---

# LimitaÃ§Ãµes e ReflexÃµes

Embora o LIME ajude a entender as decisÃµes do modelo, ele possui limitaÃ§Ãµes:

- Pode gerar explicaÃ§Ãµes inconsistentes se o modelo for muito nÃ£o linear.
- ExplicaÃ§Ãµes locais nÃ£o representam o comportamento global do modelo.
- A escolha dos dados vizinhos (perturbaÃ§Ãµes) pode afetar os resultados.

No entanto, a interpretabilidade:
-  *Promove confianÃ§a*
- *Atende exigÃªncias regulatÃ³rias*
- *Ajuda equipes a entender e melhorar o modelo*

---

# ExecuÃ§Ã£o do Projeto

*1. Clone o repositÃ³rio:*

git clone https://github.com/seu-usuario/explicabilidade-modelo-credito.git
cd explicabilidade-modelo-credito

*2. Instale as dependÃªncias:*


pip install -r requirements.txt

*3. execute o notebook:*

Abra o projeto_lime.ipynb no Jupyter ou Google Colab.


*4. Dataset utilizado:*

German Credit Data - UCI Machine Learning Repository

**Lembrando que, como o o projeto foi feito a partir do Google Colab, nÃ£o seria necessario baixar o arquivo manualmente, pois podemos pegar diretamente do link.**
